@incollection{burelCimpleKGContinuouslyUpdated2025,
  title = {{{CimpleKG}}: {{A Continuously Updated Knowledge Graph}} on {{Misinformation}}, {{Factors}} and {{Fact-Checks}}},
  shorttitle = {{{CimpleKG}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2024},
  author = {Burel, Grégoire and Mensio, Martino and Peskine, Youri and Troncy, Raphael and Papotti, Paolo and Alani, Harith},
  editor = {Demartini, Gianluca and Hose, Katja and Acosta, Maribel and Palmonari, Matteo and Cheng, Gong and Skaf-Molli, Hala and Ferranti, Nicolas and Hernández, Daniel and Hogan, Aidan},
  date = {2025},
  volume = {15233},
  pages = {97--114},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77847-6_6},
  url = {https://link.springer.com/10.1007/978-3-031-77847-6_6},
  urldate = {2025-08-27},
  isbn = {978-3-031-77846-9 978-3-031-77847-6},
  langid = {english},
  file = {/Users/ale/Zotero/storage/Q5IXCKQ2/Burel et al. - 2025 - CimpleKG A Continuously Updated Knowledge Graph on Misinformation, Factors and Fact-Checks.pdf}
}

@inproceedings{gangopadhyayTruthDareInvestigating2023,
  title = {Truth or {{Dare}}: {{Investigating Claims Truthfulness}} with {{ClaimsKG}}},
  shorttitle = {Truth or {{Dare}}},
  author = {Gangopadhyay, Susmita and Boland, Katarina and Dessí, Danilo and Dietze, Stefan and Fafalios, Pavlos and Tchechmedjiev, Andon and Todorov, Konstantin and Jabeen, Hajira},
  date = {2023-05-28},
  volume = {3401},
  url = {https://imt-mines-ales.hal.science/hal-04105799},
  urldate = {2025-08-27},
  abstract = {Searching and exploring online information is fundamental for our society. However, it is common to find inaccurate information on the Internet, that can quickly spread and be hard to identify. Fortunately, today, many fact-checking sources verify online information to provide online users with a means to recognize its truthfulness. These sources use different languages and scoring systems, which makes fact validation challenging and time-consuming. To address this issue, we propose a new release of ClaimsKG, a knowledge graph of about 59,580 claims, which covers 13 different fact-checking sources and provides a structured way to retrieve verified online claims. ClaimsKG is built using a pipeline that makes use of entity linking and disambiguation tools to fetch entities from DBpedia and an ad-hoc scoring normalization system. ClaimsKG is used as a showcase to provide the public with interesting and verified information about events of our times.},
  eventtitle = {{{D2R2}} 2023 - 2nd {{International Workshop}} on {{Linked Data-driven Resilience Research}}},
  langid = {english},
  file = {/Users/ale/Zotero/storage/7RZHMN23/Gangopadhyay et al. - 2023 - Truth or Dare Investigating Claims Truthfulness with ClaimsKG.pdf}
}

@inproceedings{jiangKGAgentEfficientAutonomous2025,
  title = {{{KG-Agent}}: {{An Efficient Autonomous Agent Framework}} for {{Complex Reasoning}} over {{Knowledge Graph}}},
  shorttitle = {{{KG-Agent}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jiang, Jinhao and Zhou, Kun and Zhao, Xin and Song, Yang and Zhu, Chen and Zhu, Hengshu and Wen, Ji-Rong},
  date = {2025},
  pages = {9505--9523},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.468},
  url = {https://aclanthology.org/2025.acl-long.468},
  urldate = {2025-08-27},
  eventtitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {/Users/ale/Zotero/storage/ZIAELT3L/Jiang et al. - 2025 - KG-Agent An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph.pdf}
}

@article{linBioKGBenchKnowledgeGraph2024,
  title = {{{BioKGBench}}: {{A Knowledge Graph Checking Benchmark}} of {{AI Agent}} for {{Biomedical Science}}},
  shorttitle = {{{BioKGBench}}},
  author = {Lin, Xinna and Ma, Siqi and Shan, Junjie and Zhang, Xiaojing and Hu, Shell Xu and Guo, Tiannan and Li, Stan Z. and Yu, Kaicheng},
  date = {2024-10-04},
  url = {https://openreview.net/forum?id=I1MKOjNVup},
  urldate = {2025-08-27},
  abstract = {Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, researchers typically rely on direct Question-Answering (QA) to the LLM itself or through biomedical experiments. How to benchmark biomedical agents precisely from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from scientists’ crucial ability to understand the literature and introduce BioKGBench. In contrast to traditional evaluation benchmarks that focus solely on factual QA, where the LLMs are known to have hallucination issues, we first disentangle “Understanding Literature” into two atomic abilities: i) “Understanding” the unstructured text from research papers by performing scientific claim verification, and ii) interacting with structured Knowledge-Graphs for Question-Answering (KGQA) as a form of “Literature” grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify factual errors in existing large-scale knowledge graphs. We collect over two thousand data points for the two atomic tasks and 225 high-quality annotated samples for the agent task. Surprisingly, we find that state-of-the-art general and biomedical agents have either failed or performed inferiorly on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors, which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach.},
  langid = {english},
  file = {/Users/ale/Zotero/storage/JBQJF9QG/Lin et al. - 2024 - BioKGBench A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science.pdf}
}

@inproceedings{thibaultGuideMisinformationDetection2025,
  title = {A {{Guide}} to {{Misinformation Detection Data}} and {{Evaluation}}},
  booktitle = {Proceedings of the 31st {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining V}}.2},
  author = {Thibault, Camille and Tian, Jacob-Junqi and Péloquin-Skulski, Gabrielle and Curtis, Taylor Lynn and Zhou, James and Laflamme, Florence and Guan, Luke Yuxiang and Rabbany, Reihaneh and Godbout, Jean-François and Pelrine, Kellin},
  date = {2025-08-03},
  pages = {5801--5809},
  publisher = {ACM},
  location = {Toronto ON Canada},
  doi = {10.1145/3711896.3737437},
  url = {https://dl.acm.org/doi/10.1145/3711896.3737437},
  urldate = {2025-08-27},
  eventtitle = {{{KDD}} '25: {{The}} 31st {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {979-8-4007-1454-2},
  langid = {english},
  file = {/Users/ale/Zotero/storage/UFM2J3X2/Thibault et al. - 2025 - A Guide to Misinformation Detection Data and Evaluation.pdf}
}
