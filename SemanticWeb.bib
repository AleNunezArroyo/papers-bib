@incollection{burelCimpleKGContinuouslyUpdated2025,
  title = {{{CimpleKG}}: {{A Continuously Updated Knowledge Graph}} on {{Misinformation}}, {{Factors}} and {{Fact-Checks}}},
  shorttitle = {{{CimpleKG}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2024},
  author = {Burel, Grégoire and Mensio, Martino and Peskine, Youri and Troncy, Raphael and Papotti, Paolo and Alani, Harith},
  editor = {Demartini, Gianluca and Hose, Katja and Acosta, Maribel and Palmonari, Matteo and Cheng, Gong and Skaf-Molli, Hala and Ferranti, Nicolas and Hernández, Daniel and Hogan, Aidan},
  date = {2025},
  volume = {15233},
  pages = {97--114},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-77847-6_6},
  url = {https://link.springer.com/10.1007/978-3-031-77847-6_6},
  urldate = {2025-08-27},
  isbn = {978-3-031-77846-9 978-3-031-77847-6},
  langid = {english},
  file = {/Users/ale/Zotero/storage/Q5IXCKQ2/Burel et al. - 2025 - CimpleKG A Continuously Updated Knowledge Graph on Misinformation, Factors and Fact-Checks.pdf}
}

@inproceedings{fatahibayatFactBenchDynamicBenchmark2025,
  title = {{{FactBench}}: {{A Dynamic Benchmark}} for {{In-the-Wild Language Model Factuality Evaluation}}},
  shorttitle = {{{FactBench}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Fatahi Bayat, Farima and Zhang, Lechen and Munir, Sheza and Wang, Lu},
  editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  date = {2025-07},
  pages = {33090--33110},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.1587},
  url = {https://aclanthology.org/2025.acl-long.1587/},
  urldate = {2025-08-27},
  abstract = {The rapid adoption of language models (LMs) across diverse applications has raised concerns about their factuality, i.e., their consistency with real-world facts. We introduce VERIFY, an evidence-based evaluation pipeline that measures LMs' factuality in real-world user interactions. VERIFY considers the verifiability of LM-generated content and categorizes content units as Supported, Unsupported, or Undecidable based on Web-retrieved evidence. Importantly, factuality judgment by VERIFY more strongly correlates with human evaluations than existing methods. Using VERIFY, we identify “hallucination prompts,” i.e., those that frequently elicit factual errors in LM responses. These prompts form FactBench, a dataset of 1K prompts spanning 150 topics and tiered into Easy, Moderate, and Hard prompts. We benchmark widely-used openweight and proprietary LMs from six families, yielding three key findings: (i) LMs' factual precision declines from Easy to Hard prompts, (ii) factuality does not necessarily improve with scale; Llama3.1-405B-Instruct performs comparably to or worse than its 70B variant, and (iii) Gemini1.5-Pro shows a notably higher refusal rate, with over-refusal in 25\% of cases.},
  eventtitle = {{{ACL}} 2025},
  isbn = {979-8-89176-251-0}
}

@inproceedings{gangopadhyayTruthDareInvestigating2023,
  title = {Truth or {{Dare}}: {{Investigating Claims Truthfulness}} with {{ClaimsKG}}},
  shorttitle = {Truth or {{Dare}}},
  author = {Gangopadhyay, Susmita and Boland, Katarina and Dessí, Danilo and Dietze, Stefan and Fafalios, Pavlos and Tchechmedjiev, Andon and Todorov, Konstantin and Jabeen, Hajira},
  date = {2023-05-28},
  volume = {3401},
  url = {https://imt-mines-ales.hal.science/hal-04105799},
  urldate = {2025-08-27},
  abstract = {Searching and exploring online information is fundamental for our society. However, it is common to find inaccurate information on the Internet, that can quickly spread and be hard to identify. Fortunately, today, many fact-checking sources verify online information to provide online users with a means to recognize its truthfulness. These sources use different languages and scoring systems, which makes fact validation challenging and time-consuming. To address this issue, we propose a new release of ClaimsKG, a knowledge graph of about 59,580 claims, which covers 13 different fact-checking sources and provides a structured way to retrieve verified online claims. ClaimsKG is built using a pipeline that makes use of entity linking and disambiguation tools to fetch entities from DBpedia and an ad-hoc scoring normalization system. ClaimsKG is used as a showcase to provide the public with interesting and verified information about events of our times.},
  eventtitle = {{{D2R2}} 2023 - 2nd {{International Workshop}} on {{Linked Data-driven Resilience Research}}},
  langid = {english},
  file = {/Users/ale/Zotero/storage/7RZHMN23/Gangopadhyay et al. - 2023 - Truth or Dare Investigating Claims Truthfulness with ClaimsKG.pdf}
}

@inproceedings{jiangKGAgentEfficientAutonomous2025,
  title = {{{KG-Agent}}: {{An Efficient Autonomous Agent Framework}} for {{Complex Reasoning}} over {{Knowledge Graph}}},
  shorttitle = {{{KG-Agent}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jiang, Jinhao and Zhou, Kun and Zhao, Xin and Song, Yang and Zhu, Chen and Zhu, Hengshu and Wen, Ji-Rong},
  date = {2025},
  pages = {9505--9523},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.468},
  url = {https://aclanthology.org/2025.acl-long.468},
  urldate = {2025-08-27},
  eventtitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {/Users/ale/Zotero/storage/ZIAELT3L/Jiang et al. - 2025 - KG-Agent An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph.pdf}
}

@online{linBioKGBenchKnowledgeGraph2024,
  title = {{{BioKGBench}}: {{A Knowledge Graph Checking Benchmark}} of {{AI Agent}} for {{Biomedical Science}}},
  shorttitle = {{{BioKGBench}}},
  author = {Lin, Xinna and Ma, Siqi and Shan, Junjie and Zhang, Xiaojing and Hu, Shell Xu and Guo, Tiannan and Li, Stan Z. and Yu, Kaicheng},
  date = {2024-06-29},
  eprint = {2407.00466},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.00466},
  url = {http://arxiv.org/abs/2407.00466},
  urldate = {2025-08-27},
  abstract = {Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle "Understanding Literature" into two atomic abilities, i) "Understanding" the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of "Literature" grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task. Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ale/Zotero/storage/G8JQTD8B/Lin et al. - 2024 - BioKGBench A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science.pdf;/Users/ale/Zotero/storage/RMWSITTG/2407.html}
}

@inproceedings{shamiFactVerificationKnowledge2025,
  title = {Fact {{Verification}} in {{Knowledge Graphs Using LLMs}}},
  booktitle = {Proceedings of the 48th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Shami, Farzad and Marchesin, Stefano and Silvello, Gianmaria},
  date = {2025-07-13},
  series = {{{SIGIR}} '25},
  pages = {3985--3989},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3726302.3730142},
  url = {https://doi.org/10.1145/3726302.3730142},
  urldate = {2025-08-27},
  abstract = {Automated fact-checking systems often struggle with trustworthiness, as they lack transparency in their reasoning processes and fail to handle relationships in data. This work presents FactCheck, a fact verification system topped by a web platform that shows how Large Language Models (LLMs) can be collectively used to verify facts within Knowledge Graphs (KGs). While the underlying verification engine implements a system that combines Retrieval Augmented Generation (RAG) with an ensemble of LLMs to validate KG facts, the platform focuses on making the results of this complex process as transparent and accessible as possible. Users can explore how different models interpret the same evidence, compare their reasoning patterns, and understand the factors that lead to the final verification result. The platform supports technical users who want to analyze the model behavior and general users who need to verify whether the facts in the dataset are correct.},
  isbn = {979-8-4007-1592-1}
}

@inproceedings{thibaultGuideMisinformationDetection2025,
  title = {A {{Guide}} to {{Misinformation Detection Data}} and {{Evaluation}}},
  booktitle = {Proceedings of the 31st {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining V}}.2},
  author = {Thibault, Camille and Tian, Jacob-Junqi and Péloquin-Skulski, Gabrielle and Curtis, Taylor Lynn and Zhou, James and Laflamme, Florence and Guan, Luke Yuxiang and Rabbany, Reihaneh and Godbout, Jean-François and Pelrine, Kellin},
  date = {2025-08-03},
  pages = {5801--5809},
  publisher = {ACM},
  location = {Toronto ON Canada},
  doi = {10.1145/3711896.3737437},
  url = {https://dl.acm.org/doi/10.1145/3711896.3737437},
  urldate = {2025-08-27},
  eventtitle = {{{KDD}} '25: {{The}} 31st {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {979-8-4007-1454-2},
  langid = {english},
  file = {/Users/ale/Zotero/storage/UFM2J3X2/Thibault et al. - 2025 - A Guide to Misinformation Detection Data and Evaluation.pdf}
}
