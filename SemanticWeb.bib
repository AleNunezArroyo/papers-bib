@inproceedings{jiangKGAgentEfficientAutonomous2025,
  title = {{{KG-Agent}}: {{An Efficient Autonomous Agent Framework}} for {{Complex Reasoning}} over {{Knowledge Graph}}},
  shorttitle = {{{KG-Agent}}},
  booktitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jiang, Jinhao and Zhou, Kun and Zhao, Xin and Song, Yang and Zhu, Chen and Zhu, Hengshu and Wen, Ji-Rong},
  date = {2025},
  pages = {9505--9523},
  publisher = {Association for Computational Linguistics},
  location = {Vienna, Austria},
  doi = {10.18653/v1/2025.acl-long.468},
  url = {https://aclanthology.org/2025.acl-long.468},
  urldate = {2025-08-27},
  eventtitle = {Proceedings of the 63rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {/Users/ale/Zotero/storage/ZIAELT3L/Jiang et al. - 2025 - KG-Agent An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph.pdf}
}

@online{nassarSmolDoclingUltracompactVisionlanguage2025,
  title = {{{SmolDocling}}: {{An}} Ultra-Compact Vision-Language Model for End-to-End Multi-Modal Document Conversion},
  shorttitle = {{{SmolDocling}}},
  author = {Nassar, Ahmed and Marafioti, Andres and Omenetti, Matteo and Lysak, Maksym and Livathinos, Nikolaos and Auer, Christoph and Morin, Lucas and family=Lima, given=Rafael Teixeira, prefix=de, useprefix=true and Kim, Yusik and Gurbuz, A. Said and Dolfi, Michele and Farr√©, Miquel and Staar, Peter W. J.},
  date = {2025},
  doi = {10.48550/ARXIV.2503.11576},
  url = {https://arxiv.org/abs/2503.11576},
  urldate = {2025-07-05},
  abstract = {We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.},
  pubstate = {prepublished},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/ale/Zotero/storage/8QHGM54R/2503.11576v1.pdf}
}
